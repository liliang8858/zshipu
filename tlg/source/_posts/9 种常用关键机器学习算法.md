
title: 9 种常用关键机器学习算法
author: 知识铺
date: 2020-06-21 11:17:12
tags:
---
  

机器学习正在改变世界。谷歌使用机器学习向用户推荐搜索结果。Netflix 使用它推荐电影供您观看。Facebook 使用机器学习来推荐您可能认识的人。

机器学习从未像现在这样重要。同时，理解机器学习也很难。田野里充满了行话。不同的 ML 算法数量每年都在增长。

本文将向您介绍机器学习领域的基本概念。更具体地说，我们将讨论当今 9 个最重要的机器学习算法背后的基本概念。

# 推荐系统

### 什么是推荐系统？

[建议系统](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/introduction-recommendation-systems/)用于在数据集中查找类似的条目。

也许最常见的建议真实示例存在于 Netflix 中。更具体地说，其视频流媒体服务将推荐基于你已经观看的内容的电影和电视节目。

另一个推荐系统是Facebook的"你可能认识的人"功能，它建议你可能的朋友，根据你现有的朋友名单。

充分开发和部署的建议系统极其复杂。它们也非常耗费资源。

### 推荐系统和线性代数

成熟的推荐系统需要线性代数的深层背景，才能从头开始构建。

因此，如果您以前从未研究过线性代数，则本节中可能有些概念无法理解。

不过，别担心——学习的 Python 库使得构建推荐系统变得非常简单。S0 你不需要太多的线性代数背景来构建真实世界的建议系统。

### 推荐系统如何工作？

推荐系统主要有两种类型：

*   基于内容的建议系统
*   协作筛选建议系统

基于内容的推荐系统根据已使用的项目的相似性为您提供建议。他们完全按照你期望的推荐系统来做。

协作筛选建议系统根据用户与项目交互的知识生成建议。换句话说，他们利用了人群的智慧。（因此，其名称中的术语"协作"。

在现实世界中，协作筛选建议系统比基于内容的系统更常见。这主要是因为它们通常能产生更好的结果。一些从业者还发现协作筛选建议系统更易于理解。

协作筛选建议系统还具有缺少基于内容的系统的独特功能。也就是说，他们有能力自己学习功能。

这意味着他们甚至可以开始根据您甚至未告诉他们要考虑的属性来识别项之间的相似性。

协作筛选中有两个子类别：

*   基于内存的协作筛选
*   基于模型的协作筛选

在机器学习中取得成功，您无需了解这两种类型的协作筛选建议系统之间的区别。这足以识别存在多种类型。

### 章节总结

以下是我们在本教程中讨论的关于建议系统的简要摘要：

*   现实世界中推荐制度的例子
*   不同类型的推荐系统，以及协作筛选系统如何比基于内容的建议系统更常用
*   推荐系统与线性代数的关系

# 线性回归

[线性回归](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/linear-regression-python/)用于根据另一组值的值预测某些值。```y``````x```

### 线性回归的历史

线性回归是由[弗朗西斯·加尔顿](https://zshipu.com/t?url=https://en.wikipedia.org/wiki/Francis_Galton)在19世纪创建的。

高尔顿是研究父母和孩子之间关系的科学家。更具体地说，高尔顿正在调查父亲身高和儿子身高之间的关系。

高尔顿的第一个发现是，儿子往往和父亲一样高。这并不奇怪。

后来，加尔顿发现了一些更有趣的东西。儿子的身高往往__更接近于所有人的总平均身高__，而不是他自己的父亲__。__

加尔顿给这种现象起一个名称：****回归****。具体来说，他说"父亲的儿子的身高倾向于倒退（或向）平均（平均）高度漂移"。

这导致了整个统计和机器学习领域称为回归。

### 线性回归数学

创建回归模型时，我们尝试的只是绘制一条尽可能接近数据集中每个点的线。

典型的示例是线性回归的"最小平方法"，该方法仅计算向上和向下方向的直线的接近度。

下面是一个示例，可帮助说明这一点：

![An example of the math behind least squares regression](https://nickmccullum.com/images/python-machine-learning/introduction-linear-regression/least-squares-regression.gif)

创建回归模型时，最终产品是一个方程，可用于预测 x 值的 y 值，而无需事先实际知道 y 值。

# 逻辑回归

[逻辑回归](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/logistic-regression-python/)类似于线性回归，只不过它不是计算数值，而是估计数据点属于哪个_类别_。```y```

### 什么是逻辑回归？

逻辑回归是一种机器学习模型，用于解决分类问题。

以下是机器学习分类问题的一些示例：

*   垃圾邮件（垃圾邮件还是垃圾邮件？
*   汽车保险索赔（注销或修理？
*   疾病诊断

每个分类问题都有两个类别，这使得它们成为**二进制分类**问题的示例。

逻辑回归非常适合解决**二进制分类**问题 - 我们只是为每个类别指定值和分别。```0``````1```

为什么我们需要逻辑回归？因为不能使用线性回归模型进行二进制分类预测。它不会导致良好的拟合，因为您尝试通过只有两个可能的值的数据集拟合直线。

此图像可以帮助您理解为什么线性回归模型不适合二进制分类问题：

![Linear Regression Classification](https://nickmccullum.com/images/python-machine-learning/introduction-logistic-regression/linear-regression-classification.png)

在此图像中， 表示肿瘤是恶性的概率.相反，该值表示肿瘤不是恶性肿瘤的概率。如您所见，线性回归模型在预测数据集中大多数观测值的此概率方面做得很差。```y-axis``````1-y```

这就是为什么逻辑回归模型很有用。它们有一个弯曲到他们的最佳拟合线，这使得他们更适合预测分类数据。

下面是一个使用相同训练数据将线性回归模型与逻辑回归模型进行比较的示例：

![Linear Regression vs Logistic Regression](https://nickmccullum.com/images/python-machine-learning/introduction-logistic-regression/linear-vs-logistic-regression.png)
### 西格莫伊德函数

逻辑回归模型在其曲线中具有折弯的原因是因为它没有使用线性方程计算。相反，逻辑回归模型是使用 Sigmoid 函数构建的（由于逻辑回归中使用，因此也称为逻辑函数）。

您不必记住[Sigmoid 函数](https://zshipu.com/t?url=https://en.wikipedia.org/wiki/Sigmoid_function)即可在机器学习中取得成功。话虽如此，对它的外观有一些了解是有用的。

公式如下所示：

![The Sigmoid Equation](https://nickmccullum.com/images/python-machine-learning/introduction-logistic-regression/sigmoid-equation.png)

Sigmoid 函数的主要特征值得理解：无论您向它传递什么价值，它始终都会在 0 和 1 之间生成输出。

### 使用逻辑回归模型进行预测

要使用线性回归模型进行预测，通常需要指定截止点。此截止点通常是 。```0.5```

让我们用我们早期图像中的癌症诊断示例来在实践中看到这一原则。如果逻辑回归模型输出的值低于 0.5，则数据点被归类为非恶性肿瘤。同样，如果 Sigmoid 函数输出的值高于 0.5，则肿瘤将归类为恶性肿瘤。

### 使用混淆矩阵测量逻辑回归性能

混淆矩阵可用作比较机器学习中真实正值、真负数、误报和假负数的工具。

当用于测量逻辑回归模型的性能时，混淆矩阵特别有用。下面是一个示例，演示如何使用混淆矩阵：

![An Example Confusion Matrix](https://nickmccullum.com/images/python-machine-learning/introduction-logistic-regression/confusion-matrix.png)在此图中，TN 代表"真负"，FN 代表"假负"。FP 代表"误报"，TP 代表"真实正"。

混淆矩阵可用于评估模型在混淆矩阵的特定象限中是否特别弱。例如，它可能有异常数量的误报。

在某些应用程序中，确保模型在混淆矩阵的异常危险区域中表现良好也很有帮助。

例如，在这个癌症示例中，您希望非常确定模型的假阴性率不会很高，因为这将表明某人的恶性肿瘤被错误地归类为非恶性肿瘤。

### 章节总结

在本节中，您首次接触了逻辑回归机器学习模型。

以下是您学到的有关逻辑回归的简要摘要：

*   使用逻辑回归模型解决的分类问题类型
*   物流函数（也称为 Sigmoid 函数）始终输出介于 0 和 1 之间的值
*   如何使用截止点使用逻辑回归机器学习模型进行预测
*   为什么混淆矩阵可用于测量逻辑回归模型的性能

# K-最近邻域

[K 最近邻域](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/k-nearest-neighbors-python/)算法可以帮助您解决分类问题，其中有两个以上类别。

### 什么是 K-最近邻域算法？

K 最近邻域算法是基于简单原理的分类算法。事实上，这个原则是如此简单，最好通过实例来理解它。

想象一下，你有关于足球运动员和篮球运动员身高和体重的数据。K 近邻算法可用于预测新运动员是足球运动员还是篮球运动员。

为此，K 最近的邻域算法标识最接近新观测点的数据点。```K```

下图以 K 值的 K 值显示此情况，K 值为 ：```3```

![A visualization of k nearest neighbors](https://nickmccullum.com/images/python-machine-learning/introduction-k-nearest-neighbors/k-nearest-neighbors.jpg)

在此图像中，足球运动员被标记为蓝色数据点，篮球运动员标记为橙色点。我们尝试分类的数据点标记为绿色。

由于新数据点的壁橱数据点中的大部分（3个中的2个）是蓝色足球运动员，因此K最近的邻域算法将预测新数据点也是足球运动员。

### 构建 K-最近邻域算法的步骤

构建 K 最近邻域算法的一般步骤是：

1.  存储所有数据
2.  计算[欧几里登](https://zshipu.com/t?url=https://en.wikipedia.org/wiki/Euclidean_distance)距离的新数据点到数据集中的所有其他点```x```
3.  按与```x```
4.  使用与大多数最近数据点相同的类别进行预测```K``````x```

### K-最近邻域算法中 K 的重要性

尽管从一开始可能并不明显，但更改 K-最近邻域算法中 的值将更改新点分配给哪个类别。```K```

更具体地说，值非常低将导致模型完全预测您的训练数据，并预测测试数据不足。同样，值过高会使模型不必要地复杂。```K``````K```

以下可视化效果很好地说明了这一点：

![K value and error rates](https://nickmccullum.com/images/python-machine-learning/introduction-k-nearest-neighbors/k-value-error-rates.png)
### K-最近邻域算法的优缺点

最后，对 K-最近邻域算法的介绍，我想简要地讨论使用此模型的一些利弊。

以下是 K 最接近邻域算法的一些主要优点：

*   该算法简单易懂
*   根据新的训练数据对模型进行培训是微不足道的
*   它适用于分类问题中的任意数量的类别
*   可以轻松地向数据集添加更多数据
*   模型仅接受两个参数：以及您要使用的距离指标（通常为欧几里得距离）```K```

同样，以下是算法的一些主要缺点：

*   进行预测的计算成本很高，因为您需要对整个数据集进行排序
*   它不适用于分类功能

### 章节总结

以下是您刚刚了解到的关于 k-最近邻域算法的简要摘要：

*   K 近邻算法可以解决的分类问题（足球运动员与篮球运动员）的示例
*   K 最近邻域如何使用相邻数据点的欧几里得距离来预测新数据点属于哪个类别
*   为什么进行预测的价值```K```
*   使用 K 最近邻域算法的优缺点

# 决策树和随机林

[决策树和随机林](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/decision-trees-random-forests-python/)都是树方法的示例。

更具体地说，决策树是机器学习模型，用于通过逐个循环浏览数据集中的每个要素来进行预测。随机林是决策树的集中，它们使用数据集中要素的随机顺序。

### 什么是树方法？

在深入探讨机器学习中树方法的理论基础之前，先从示例开始是很有帮助的。

想象一下你每个星期一打篮球。此外，你总是邀请同一个朋友来和你一起玩。

有时朋友真的来了。有时他们没有。

决定是否来取决于许多因素，如天气，温度，风和疲劳。您开始注意到这些功能，并开始跟踪它们，同时您朋友决定是否播放。

您可以使用这些数据来预测你的朋友是否会出现打篮球。可以使用的一种技术是决策树。以下是此决策树的外观：

![An example of a decision tree](https://nickmccullum.com/images/python-machine-learning/introduction-decision-tree/decision-tree.png)

每个决策树都有两种类型的元素：

*   ```Nodes```：树根据某些属性的值拆分的位置
*   ```Edges```：拆分到下一个节点的结果

您可以在上面的图像中看到 ， 和 的 节点。每个属性的每个潜在值都有一个边。```outlook``````humidity``````windy```

下面是在继续操作之前应了解的另外两个决策树术语：

*   ```Root```：执行第一次拆分的节点
*   ```Leaves```：预测最终结果的终端节点

现在，您对决策树有了基本的了解。我们将在下一节中了解如何从头开始构建决策树。

### 如何从头开始构建决策树

建立决策树比你想象的要难。这是因为决定要拆分数据的功能（这是属于[熵](https://zshipu.com/t?url=https://en.wikipedia.org/wiki/Entropy)和信息[增益](https://zshipu.com/t?url=https://machinelearningmastery.com/information-gain-and-mutual-information/#:~:text=Information+gain+is+the+reduction,before+and+after+a+transformation.)领域的主题）是一个数学上复杂的问题。

为了解决这个问题，机器学习从业者通常使用许多决策树，使用选择为拆分的要素的随机样本。

换句话说，每次拆分时，都会为每个树选择新的要素随机样本。这种技术称为**随机林**。

通常，从业者通常选择要素随机样本（表示）的大小作为数据集（表示）中总要素数的平方根。简洁，是 的平方根，然后从 随机选择特定要素。```m``````p``````m``````p``````m```

如果这现在不完全有意义，不要担心。当您最终构建第一个随机林模型时，它将更加清晰。

### 使用随机林的好处

假设您正在使用具有非常强大功能的数据集。换句话说，数据集有一个功能比数据集中的其他功能更具有预测最终结果。

如果要手动构建决策树，则使用此功能作为决策树的顶部拆分是有意义的。这意味着您将有多个预测高度相关的树。

我们希望避免这种情况，因为取高度相关变量的平均值不会显著降低方差。通过随机选择随机林中每个树的特征，树变得不相关，并减小生成的模型的方差。这种相互性是使用随机林而不是手工决策树的主要优点

### 章节总结

本文简要总结了您学到的关于决策树和随机林的知识：

*   可以使用决策树预测的问题示例
*   决策树的元素： 、 、 和```nodes``````edges``````roots``````leaves```
*   如何随机采样决策树特征，使我们能够构建一个随机林
*   为什么使用随机林来关联变量有助于降低最终模型的方差

# 支持矢量机

[支持向量机](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/support-vector-machines-python/)是分类算法（尽管从技术上讲，它们也可用于解决回归问题），它们通过切片划分类别之间的最广泛差距将数据集划分为多个类别。这一概念将在一瞬间通过可视化来更加清晰。

### 什么是支持向量机？

[支持向量机](https://zshipu.com/t?url=https://en.wikipedia.org/wiki/Support_vector_machine)（简称 SVM）是受监督的机器学习模型，具有分析数据和识别模式的相关学习算法。

支持向量机可用于分类问题和回归问题。在本文中，我们将专门研究支持向量机用于解决分类问题。

### 支持矢量机的工作原理是什么？

让我们深入了解支持向量机的真正工作方式。

给定一组训练示例（每个示例都标记为属于两个类别之一），支持向量机器训练算法可生成模型。此模型将新示例分配给两个类别之一。这使得支持向量机成为非概率二进制线性分类器。

SVM 使用几何进行分类预测。

更具体地说，SVM 模型将数据点映射为空间中的点，并将单独的类别进行划分，以便它们除以尽可能宽的开放间隙。根据它们所属的间隙的哪一侧，预测新数据点属于一个类别。

下面是一个示例可视化，可帮助您了解支持向量机背后的直觉：

![](https://www.freecodecamp.org/news/content/images/2020/06/image-57.png)

正如您所看到的，如果新数据点落在绿线的左侧，它将使用红色类别标记。同样，如果新数据点落在绿线的右侧，它将被贴上属于蓝色类别的标签。

这条绿线被称为**超平面**，它是支持向量机算法的重要词汇。

让我们来看看支持向量机的不同可视表示形式：

![](https://www.freecodecamp.org/news/content/images/2020/06/image-58.png)

在此图中，超平面标记为**最佳超平面**。支持向量机理论将**最佳超平面**定义为最大化每个类别最近数据点之间的边距。

如您所见，边距线实际上触及三个数据点 - 两个来自红色类别，一个来自蓝色类别。这些接触边距线的数据点称为**支持向量**，是支持向量机从中获取其名称的位置。

### 章节总结

以下是您刚刚了解到的支持向量机的简要摘要：

*   支持向量机是受监督机器学习算法的一个示例
*   支持向量机可用于解决分类和回归问题
*   支持向量机如何使用超平面对数据点进行分类，该**超平面**可最大化数据集中类别之间的差值
*   在支持向量机中触摸边距线的数据点称为**支持向量**。这些数据点是支持向量机从中派生其名称的位置。

# K-意味着聚类

[K-意味着聚类](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/k-means-clustering-python/)是一种机器学习算法，允许您识别数据集中类似数据段。

### 什么是 K-意味着聚类？

K-意味着聚类是一种不受监督的机器学习算法。

这意味着它占用了未标记的数据，并尝试将数据中的相似观测组分组在一起。

K-意味着聚类算法对于解决实际问题非常有用。以下是此机器学习模型的一些用例：

*   营销团队的客户细分
*   文档分类
*   亚马逊、UPS 或联邦快递等公司的交付路线优化
*   识别和应对城市内的犯罪中心
*   专业体育分析
*   预测和预防网络犯罪

K 表示聚类算法的主要目标是将数据集划分为不同的组，以便每个组中的观测值彼此相似。

下面是实际外观的视觉表示形式：

![A Visualization of a K Means Clustering Algorithm](https://nickmccullum.com/images/python-machine-learning/introduction-k-means-clustering/k-means-clustering.png)

我们将在本教程的下一节中探讨 K 表示聚类背后的数学。

### K-意味着聚类算法的工作原理是什么？

运行 K-表示聚类算法的第一步是选择要将数据划分为的群集数。此群集数是算法名称中引用的值。```K```

在 K-表示聚类算法中选择值是一个重要的选择。在本文中，我们将更多地讨论如何选择后面的正确值。```K``````K```

接下来，必须随机将数据集中的每个点分配给随机群集。这提供了我们的初始赋值，然后运行以下迭代，直到群集停止更改：

*   通过取取该群集内点的平均矢量来计算每个群集的质心
*   将每个数据点重新分配给具有最近质心的群集

下面是一个动画，说明对于值 为 的 K-表示聚类算法来说，这在实践中是如何工作的。您可以看到由黑色字符表示的每个群集的质心。```K``````3``````+```

![A Visualization of a K Means Clustering Algorithm](https://nickmccullum.com/images/python-machine-learning/introduction-k-means-clustering/k-means-iteration.gif)

如您所见，此迭代将继续，直到群集停止更改 - 这意味着数据点不再分配给新的群集。

### 为 K 选择正确的 K 值意味着聚类算法

为 K-意味着聚类算法选择适当的值实际上相当困难。选择"最佳"值没有"正确"答案。```K``````K```

机器学习从业者经常使用的一种方法称为**肘部方法**。

要使用弯头方法，您需要做的第一件事是计算一组值的 K 表示聚类算法的平方误差 （SSE） 的总和。K 中的 SSE 表示聚类算法定义为群集中每个数据点与群集的质心之间的平方距离之和。```K```

作为此步骤的示例，您可以计算 SSE 的值， 和 。```K``````2``````4``````6``````8``````10```

接下来，您需要针对这些不同的值生成 SSE 的绘图。您将看到错误会随着值的增加而减小。```K``````K```

这是有道理的 - 在数据集中创建的类别越多，每个数据点都越接近其特定群集的中心。

话虽如此，肘部法背后的理念是选择一个值，即上证所突然放慢下跌速度。此突然减少在图形中生成 。```K``````elbow```

例如，下面是 SSE 反对 的图表。在这种情况下，弯头方法建议使用近似 值。```K``````K``````6```

![A Visualization of a K Means Clustering Algorithm](https://nickmccullum.com/images/python-machine-learning/introduction-k-means-clustering/elbow-method.png)

重要的是，只是对使用价值的估计。K-表示聚类算法中永远不会有"最佳"值。与机器学习领域的许多内容一样，这是一个高度依赖于情况的决策。```6``````K``````K```

### 章节总结

以下是您在本文中学到的内容的简要摘要：

*   K-意味着聚类算法能够解决的无监督机器学习问题示例
*   K-意味着聚类算法的基本原理
*   K-意味着聚类算法的工作原理
*   如何使用弯头方法在 K 表示聚类模型中选择 适当的值```K```

# 主要组件分析

[主要组件分析](https://zshipu.com/t?url=https://nickmccullum.com/python-machine-learning/principal-component-analysis-python/)用于将多要素数据集转换为具有较少功能的转换数据集，其中每个新功能都是预先存在的要素的线性组合。此转换数据集旨在以更简单得多来解释原始数据集的大部分差异。

### 什么是主要组件分析？

主要组件分析是一种机器学习技术，用于检查变量集之间的相互关系。

说的不同，主要成分分析研究变量集，以便确定这些变量的基础结构。

主要成分分析有时称为****因子分析****。

基于此描述，您可能会认为主要分量分析与线性回归非常相似。

事实并非如此。事实上，这两种技术有一些重要的区别。

### 线性回归与主分量分析的区别

线性回归确定数据集的最佳拟合线。主组件分析确定几个最适合数据集的正交行。

如果您不熟悉正**交**一词，则只表示线彼此成直角（90 度），如地图上的北、东、南和西。

让我们考虑一个示例，以帮助您更好地理解这一点。

![A principal component analysis](https://nickmccullum.com/images/python-machine-learning/introduction-principal-component-analysis/principal-component-analysis.png)

查看此图像中的轴标签。

在此图像中，x 轴主组件示例数据集中 73% 的方差。y 轴主组件解释了数据集中大约 23% 的方差。

这意味着数据集中 4% 的方差仍无法解释。您可以通过向分析添加更多主要组件来进一步减少此数字。

### 章节总结

以下是您在本教程中学到的关于主要组件分析的简要摘要：

*   该主要组件分析试图找到确定数据集中可变性的正交因子
*   主分量分析与线性回归的区别
*   在数据集内部可视化时，正交主体组件的外观
*   添加更多主要组件可以帮助您解释数据集中的更多差异
